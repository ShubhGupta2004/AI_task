{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyP1o38rq6yu6mHur3JKfpkn",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ShubhGupta2004/AI_task/blob/main/generative_Ai_task.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "ee_NSpA86qV9"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, LSTM, Dense\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the Shakespeare dataset\n",
        "shakespeare_path = \"https://storage.googleapis.com/download.tensorflow.org/data/shakespeare.txt\"\n",
        "text = tf.keras.utils.get_file('shakespeare.txt', shakespeare_path)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "If73JEdk7Cr4",
        "outputId": "7a23c159-9c1f-4835-8d61-b8371613c69e"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/download.tensorflow.org/data/shakespeare.txt\n",
            "1115394/1115394 [==============================] - 1s 1us/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Read the dataset\n",
        "with open(text, 'r', encoding='utf-8') as f:\n",
        "    text = f.read()"
      ],
      "metadata": {
        "id": "SgCx0MwP7CeF"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Tokenize the text\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts([text])\n",
        "total_words = len(tokenizer.word_index) + 1"
      ],
      "metadata": {
        "id": "CC0c5ely7CIU"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create input sequences and corresponding labels\n",
        "input_sequences = []\n",
        "for i in range(0, len(text) - 100, 1):\n",
        "    sequence = text[i:i+100]  # Use a sequence length of 100 characters\n",
        "    input_sequences.append(sequence)"
      ],
      "metadata": {
        "id": "MBS2kB8KRvEh"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert sequences to numeric data\n",
        "sequences = tokenizer.texts_to_sequences(input_sequences)\n",
        "X = pad_sequences(sequences, maxlen=100, truncating='pre')\n",
        "y = np.array([sequences[i][-1] for i in range(len(sequences))])"
      ],
      "metadata": {
        "id": "cjJi-PEh7MHH"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Build the model\n",
        "model = Sequential()\n",
        "model.add(Embedding(total_words, 50, input_length=100))\n",
        "model.add(LSTM(100, return_sequences=True))\n",
        "model.add(LSTM(100))\n",
        "model.add(Dense(total_words, activation='softmax'))\n",
        "\n",
        "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n"
      ],
      "metadata": {
        "id": "Krlqu1_-R00w"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train the model\n",
        "model.fit(X, y, epochs=11, verbose=1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Xqn6cFz3R0m6",
        "outputId": "f13713ff-99ad-4589-f22c-e53c5138378a"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/11\n",
            "34853/34853 [==============================] - 473s 14ms/step - loss: 3.7418 - accuracy: 0.3227\n",
            "Epoch 2/11\n",
            "34853/34853 [==============================] - 461s 13ms/step - loss: 2.5344 - accuracy: 0.5879\n",
            "Epoch 3/11\n",
            "34853/34853 [==============================] - 461s 13ms/step - loss: 1.5011 - accuracy: 0.7941\n",
            "Epoch 4/11\n",
            "34853/34853 [==============================] - 461s 13ms/step - loss: 0.9436 - accuracy: 0.8658\n",
            "Epoch 5/11\n",
            "34853/34853 [==============================] - 458s 13ms/step - loss: 0.5818 - accuracy: 0.9120\n",
            "Epoch 6/11\n",
            "34853/34853 [==============================] - 460s 13ms/step - loss: 0.3701 - accuracy: 0.9420\n",
            "Epoch 7/11\n",
            "34853/34853 [==============================] - 462s 13ms/step - loss: 0.2421 - accuracy: 0.9622\n",
            "Epoch 8/11\n",
            "34853/34853 [==============================] - 463s 13ms/step - loss: 0.1607 - accuracy: 0.9749\n",
            "Epoch 9/11\n",
            "34853/34853 [==============================] - 457s 13ms/step - loss: 0.1149 - accuracy: 0.9818\n",
            "Epoch 10/11\n",
            "34853/34853 [==============================] - 455s 13ms/step - loss: 0.0878 - accuracy: 0.9857\n",
            "Epoch 11/11\n",
            "34853/34853 [==============================] - 460s 13ms/step - loss: 0.0719 - accuracy: 0.9879\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.History at 0x7fedbc19fdc0>"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate text\n",
        "seed_text = \"The sun\"\n",
        "next_words = 100\n",
        "\n",
        "for _ in range(next_words):\n",
        "    token_list = tokenizer.texts_to_sequences([seed_text])[0]\n",
        "    token_list = pad_sequences([token_list], maxlen=100, truncating='pre')\n",
        "    predicted = np.argmax(model.predict(token_list, verbose=0), axis=-1)\n",
        "    output_word = \"\"\n",
        "    for word, index in tokenizer.word_index.items():\n",
        "        if index == predicted:\n",
        "            output_word = word\n",
        "            break\n",
        "    seed_text += \" \" + output_word"
      ],
      "metadata": {
        "id": "ZEyAUeZY9ray"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(seed_text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U68BBU0aVs9e",
        "outputId": "12c0d4bc-d405-490a-ede2-b85f5d359143"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The sun sun sun sun sun sun sun sun sun sun sun sun sun sun sun sun sun sun sun sun sun sun sun sun sun sun sun sun sun sun sun sun sun sun sun sun sun sun sun sun sun sun sun sun sun sun sun sun sun sun sun sun sun sun sun sun sun sun sun sun sun sun sun sun sun sun sun sun sun sun sun sun sun sun sun sun sun sun sun sun sun sun sun sun sun sun sun sun sun sun sun sun sun sun sun sun sun sun sun sun sun\n"
          ]
        }
      ]
    }
  ]
}